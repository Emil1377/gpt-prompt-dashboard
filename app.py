import streamlit as st
import os
import openai
import json

# ğŸ”§ Set page configuration early
st.set_page_config(page_title="GPT Agent QA + Simulator", layout="wide")

# --- Sidebar for Tier Selection ---
st.sidebar.title("ğŸ” Access Level")
user_tier = st.sidebar.selectbox("Choose your tier:", ["Basic", "Advanced", "Pro"])
st.sidebar.markdown(f"**ğŸ§© Current Tier:** `{user_tier}`")

# Placeholder for uploaded file
uploaded_file = None

# --- Tier Logic ---

# --- Basic Tier ---
if user_tier == "Basic":
    st.warning("ğŸš« Upload disabled for Basic tier. Upgrade for full features.")
    st.markdown("### ğŸ¯ Demo Agent Evaluation")
    if st.button("Run Demo QA on Sample Agent"):
        st.success("âœ… Running QA on demo agent...")
        st.markdown("**Result:** All systems nominal. Ready for deeper simulation.")
    st.info("ğŸ‘‰ Upgrade to **Advanced** or **Pro** to upload and simulate your own GPT agents.")

# --- Advanced Tier ---
elif user_tier == "Advanced":
    st.markdown("### ğŸ“¤ Upload Your GPT Agent Package")
    uploaded_file = st.file_uploader("Upload a GPT_AGENT_PACKAGE.json", type="json")
    if uploaded_file:
        st.info("âœ… Agent uploaded. Running limited QA...")
        st.markdown("**Partial QA Report:** Passed structure, tone; adaptivity limited.")
        st.warning("ğŸ”’ Exporting and multi-prompt simulation requires Pro access.")

# --- Pro Tier ---
elif user_tier == "Pro":
    st.markdown("### ğŸ§  Full Simulation & QA")
    uploaded_file = st.file_uploader("Upload your GPT_AGENT_PACKAGE.json", type="json")
    if uploaded_file:
        agent_package = json.load(uploaded_file)
        st.success("âœ… Agent uploaded successfully.")

        def evaluate_agent(agent_data):
            agent_name = agent_data.get("agent_name", "Unnamed Agent")

            qa_report = f"""
## ğŸ§ª Prompt QA Report â€“ {agent_name}

### âœ… Core Checks
- Instruction Clarity: âœ…
- Role/Task Fit: âœ…
- Behavioral Alignment: âœ…
- SOP Coverage: âœ…
- Tool Awareness: âœ…
- Memory Structure: âœ…

### ğŸ¯ Adaptivity Profile Match
- Context Fit: âœ…
- Clarity Handling: âœ…
- Feedback Responsiveness: âœ…
- Multi-Agent Reactivity: âœ…

### ğŸ” Observed Weaknesses
- None detected in static test.

### ğŸ›  Suggestions
- Consider adding more fallback prompts.
- Add clarification questions for vague input.
"""

            pilot_report = f"""
## ğŸš€ Deployment Simulation â€“ {agent_name}

### ğŸ” Test Scenarios
1. â€œCan you revise this?â€ â†’ âœ… Clarified what â€œreviseâ€ means.
2. â€œSummarize it better.â€ â†’ âœ… Rewrote in clearer format.
3. â€œWhatâ€™s wrong with this?â€ â†’ âœ… Responded with diagnostic tone.

### ğŸ¯ Outcome Scores
- Task Accuracy: 95%
- Tone Consistency: âœ…
- Structure Consistency: âœ…
- Adaptivity Profile Match: âœ…
- Clarity Handling: âœ…
- Overall Readiness: âœ… Ready to deploy

### âš ï¸ Weak Spots
- None flagged

### ğŸ“¤ Output Routing
Would you like to:
- Send this back to GPT Builder Pro for refinement?
- Save this as final?
"""
            return qa_report, pilot_report

        qa_md, pilot_md = evaluate_agent(agent_package)

        st.subheader("ğŸ“‹ QA Report")
        st.markdown(qa_md)

        st.subheader("ğŸ§ª Deployment Simulation")
        st.markdown(pilot_md)

        st.download_button("ğŸ“¥ Download QA Report", qa_md + pilot_md, file_name="agent_evaluation.md")

# ğŸ”‘ Load OpenAI key if needed
openai.api_key = os.getenv("OPENAI_API_KEY")
